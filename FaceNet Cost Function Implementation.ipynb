{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:29.779497Z",
     "start_time": "2021-06-21T09:16:25.627397Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "# https://arxiv.org/pdf/1503.03832.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:32.355205Z",
     "start_time": "2021-06-21T09:16:32.344173Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LFW_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # This is a dictionary with key as class name and values as a list of images path\n",
    "        self.data = {}\n",
    "        \n",
    "        # class list i.e. IDs names\n",
    "        self.classes = []\n",
    "        \n",
    "        # home Directory of dataset \n",
    "        self.imgs_path = \"dataset/\"\n",
    "        \n",
    "        # gets all folders name inside the dataset directory\n",
    "        names_list = glob.glob(self.imgs_path + \"*\")\n",
    "        \n",
    "        #considering each folder as a unique class\n",
    "        for class_path in names_list:\n",
    "            #Spliting Class name from Path\n",
    "            class_name = class_path.split(\"\\\\\")[-1]\n",
    "            \n",
    "            # Only Select those classes with more than 1 image\n",
    "            # if class have more than one Image \n",
    "            if (len(glob.glob(class_path + \"\\\\*.jpg\")) > 1 ):\n",
    "        \n",
    "                #generating list of classes\n",
    "                self.classes.append(class_name)\n",
    "                \n",
    "                images = []\n",
    "                for img_path in glob.glob(class_path + \"\\\\*.jpg\"):\n",
    "                    images.append(img_path)\n",
    "                self.data[class_name] = images\n",
    "     \n",
    "    # return total length of data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "      \n",
    "    # This function must return a single datapoint when asked \n",
    "    # datapoint is a triplet, so it must return a triplet\n",
    "    # And i am returning a triplet \n",
    "    def __getitem__(self, idx):\n",
    "        # randomly pick two random classes\n",
    "        positive_class, negative_class = np.random.choice(self.classes, size=2, replace=False)\n",
    "        \n",
    "        # randomly pick two images from positive_class\n",
    "        anchor_image_path , positive_image_path = np.random.choice(self.data[positive_class], size=2, replace=False)\n",
    "    \n",
    "        # randomly pick one image from negative class\n",
    "        negative_image_path = np.random.choice(self.data[negative_class])\n",
    "        \n",
    "        base_path = r'C:\\Users\\Tayyab\\OneDrive\\University\\MS\\Fast\\2nd Semester\\DL\\Project\\project'\n",
    "        \n",
    "        \n",
    "        # Anchor Image read #################################################################\n",
    "        anchor_image = cv2.imread(os.path.join(base_path,anchor_image_path))\n",
    "        # Transformations on anchor\n",
    "        anchor_image = torch.from_numpy(anchor_image)\n",
    "        anchor_image = anchor_image.permute(2, 0, 1)\n",
    "        \n",
    "        #positive Image read #################################################################\n",
    "        positive_image = cv2.imread(os.path.join(base_path,positive_image_path))\n",
    "        # Transformations on positive\n",
    "        positive_image = torch.from_numpy(positive_image)\n",
    "        positive_image = positive_image.permute(2, 0, 1)\n",
    "        \n",
    "        # Negative Image read ################################################################\n",
    "        negative_image = cv2.imread(os.path.join(base_path,negative_image_path))\n",
    "        # Transformations on negative\n",
    "        negative_image = torch.from_numpy(negative_image)\n",
    "        negative_image = negative_image.permute(2, 0, 1)\n",
    "        \n",
    "        ######################################################################################\n",
    "        # create a triplet tuple\n",
    "        triplet = (anchor_image, positive_image, negative_image)  \n",
    "        \n",
    "        #returns immutable tuple\n",
    "        return triplet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:35.377728Z",
     "start_time": "2021-06-21T09:16:35.369222Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:39.166558Z",
     "start_time": "2021-06-21T09:16:36.843879Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = LFW_Dataset()\n",
    "dataLoader = DataLoader(dataset, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:41.338110Z",
     "start_time": "2021-06-21T09:16:39.871564Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZF_Net(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2))\n",
       "  (pool1): MaxPool2d(kernel_size=(3, 3), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (pool2): MaxPool2d(kernel_size=(3, 3), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=(3, 3), stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv6): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=33856, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (L2): Linear(in_features=2048, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "class ZF_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3   , out_channels = 64 , kernel_size = 7 , stride = 2 ) \n",
    "        self.pool1 = nn.MaxPool2d((3, 3), stride=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 64 , out_channels = 192  , kernel_size = 3 , stride = 2 ) \n",
    "        self.pool2 = nn.MaxPool2d((3, 3), stride=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 192  , out_channels = 384 , kernel_size = 3 , stride = 1) \n",
    "        self.pool3 = nn.MaxPool2d((3, 3), stride=1)\n",
    "\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 384 , out_channels = 256 , kernel_size = 3 , stride = 1 ) \n",
    "        self.conv5 = nn.Conv2d(in_channels = 256 , out_channels = 256 , kernel_size = 3 , stride = 1 ) \n",
    "        \n",
    "        self.conv6 = nn.Conv2d(in_channels = 256 , out_channels = 64 , kernel_size = 3 , stride = 1 )  \n",
    "        self.pool4 = nn.MaxPool2d((3, 3), stride=2)        \n",
    "        \n",
    "        self.fc1 = nn.Linear(64* 23*23, 1*32*128) \n",
    "        self.fc2 = nn.Linear(1*32*128 ,16*128)  \n",
    "        self.L2  = nn.Linear(1*16*128 , 1*128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.pool1(out)\n",
    "    \n",
    "        out = self.conv2(out)\n",
    "        out = self.pool2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.pool3(out)\n",
    "    \n",
    "        out = self.conv4(out)\n",
    "        out = self.conv5(out)\n",
    "        out = self.conv6(out)\n",
    "        \n",
    "        out = self.pool4(out)\n",
    "        \n",
    "        out = torch.flatten(out, 1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.L2(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "network = ZF_Net()\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:43.515406Z",
     "start_time": "2021-06-21T09:16:43.503390Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Defining Loss function \n",
    "reference_loss_function = nn.TripletMarginLoss(margin=2.5)\n",
    "#Defining Optimizer\n",
    "optimizer = optim.SGD(network.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-18T14:22:12.896638Z",
     "start_time": "2021-06-18T14:22:12.884639Z"
    }
   },
   "source": [
    "## Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-21T09:16:45.365951Z",
     "start_time": "2021-06-21T09:16:45.355828Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Triplet_Loss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(Triplet_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, anchor,positive, negative, margin):\n",
    "\n",
    "        # Euclidean distance\n",
    "        d_A_P = torch.sqrt(torch.sum(torch.square(anchor-positive))) # torch.cdist(anchor,positive)**2 # torch.sum(((anchor - positive)**2))\n",
    "        d_A_N = torch.sqrt(torch.sum(torch.square(anchor-negative))) #torch.cdist(anchor,negative)**2 #torch.sum(((anchor- negative)**2))\n",
    "       \n",
    "        # Calculating Loss\n",
    "        loss = torch.maximum((d_A_P - d_A_N) + margin,torch.tensor(0.0))\n",
    "        \n",
    "        # Calculating Mean so siamese Network\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "loss_function = Triplet_Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-21T09:18:08.080Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Batch:  0  Loss :  3.5032100677490234 Ref Loss :  2.610060930252075\n",
      "Epoch : 1  Batch:  1  Loss :  0.0 Ref Loss :  1.7938430309295654\n",
      "Epoch : 1  Batch:  2  Loss :  0.7141342163085938 Ref Loss :  2.3509488105773926\n",
      "Epoch : 1  Batch:  3  Loss :  0.0 Ref Loss :  5.215240955352783\n",
      "Epoch : 1  Batch:  4  Loss :  0.0 Ref Loss :  2.674360752105713\n",
      "Epoch : 1  Batch:  5  Loss :  0.0 Ref Loss :  1.8178126811981201\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,epochs):  # loop over the dataset multiple times  \n",
    "    for i, tempData in enumerate(dataLoader,0):\n",
    "        \n",
    "        anchor = tempData[0].float()\n",
    "        positive = tempData[1].float()\n",
    "        nagative = tempData[2].float()\n",
    "        \n",
    "        \n",
    "        #Step 1: Predition:  Sending All images one by one to network\n",
    "        \n",
    "        f_Anchor = network(anchor)\n",
    "        f_Positive = network(positive)\n",
    "        f_Negative = network(nagative)\n",
    "        \n",
    "        # All three outputs must be 128 x 1\n",
    "        \n",
    "        # Step 2 Loss : Sending all 3 matrixes to compute loss\n",
    "        loss = loss_function(f_Anchor ,f_Positive ,f_Negative,2.5)\n",
    "        loss_reference = reference_loss_function(f_Anchor ,f_Positive ,f_Negative)\n",
    "        \n",
    "        #step 3 Backward Pass\n",
    "        loss.backward(retain_graph=True)\n",
    "        loss_reference.backward()\n",
    "        \n",
    "        #step 4 parameters Update\n",
    "        optimizer.step()\n",
    "        \n",
    "        #step 5 Reset the gradients to zero for next forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        print(\"Epoch :\", epoch+1, ' Batch: ', i , ' Loss : ', loss.item(), 'Ref Loss : ', loss_reference.item() )\n",
    "    #Saving model after everry batch\n",
    "    PATH = './models/DLModel_{}_{}.pth'.format(epoch+1,i)\n",
    "    torch.save(network.state_dict(), PATH)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
